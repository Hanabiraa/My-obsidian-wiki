---
created: [" 07-04-2023 18:08"]
aliases: [EBM]
tags:
- article/
---

# Energy-based models

## Potential energy and [[Maximum likelihood estimation|MLE]] training problems

Потенциальная энергия - насколько вероятно засемплировать семпл из модели (если энергия низкая, то более вероятно засемплировать)
![[Pasted image 20230407180913.png]]

Сложно обучать, так как считать интеграл не очень => сложно посчитать правдоподобие

## Обучение через score mutching

![[Pasted image 20230407181448.png]]

Score function - градиент по данным от логарифма плотности данных
Score function обнуляет второе слагаемое

==> **минимизировать разность между обучаемой скор-функцией и реальной скор-функцией**

Score function нужна для
- Оценки плотности в точке
- Можно семплировать

**Иллюстрация score-mutching** - хотим чтоьы правый набор векторов был похож на левый:
![[Pasted image 20230407182059.png]]

## Denosing score mutching
- нет доступа ко всем-всем данным, только к нашим семплам
- надо обобщить score-функцию

**Способ**:
- Взять данные
- Зашумить их
- Сгенерировать неограниченно много зашумленных данных
- делаем score mutching для немножко зашумленных данных (пример - датасет MNIST с зашумленными значениями пикселей)

![[Pasted image 20230407182216.png]]

## Sliced score mutching

Преобразования - хотим избавиться от градиента log_{p_data}, расписывая квадрат разности


![[Pasted image 20230407182920.png]]

- первое слагаемое - не зависит от параметра $\theta$ => константи
- третий интеграл - несложно оценить [[методом Монте-Карло]]
- второй интеграл (предполагаем что на бесконечности вероятность встретить реальные данные стремится к нулю):
![[Pasted image 20230407183144.png]]

в итоге конструкция получается следующая:
![[Pasted image 20230407183351.png]]
- теперь можно брать батчи и методом [[методом Монте-Карло]] оценивать матожидание

![[Pasted image 20230407183430.png]]

### Проблема - считается довольно долго
Чтобы получить след градиента - должны для каждого выхода сделать [[Backpropagation]]

#### Решение
Каждая координата вектора либо -1 либо 1
Можно воспользоваться Hutchinson trace estimator:
![[Pasted image 20230407183812.png]]
- в общем сумме в матожидании будет все норм

Обернем нашу функцию:
![[Pasted image 20230407183847.png]]

Больше на нужно считать эту матрицу D\*D, можно воспользоваться Vector jacobian product:

![[Pasted image 20230407184021.png]]
- получить одно число и по нему сделать [[Backpropagation]]

## Как семплировать данные?

Динамика Ланжевина (Langevin dynamics)

![[Pasted image 20230407184151.png]]

В каждый момент времени стоим в $x_i$, прибавляем шум и значение скор-функции + есть исследовательская компонента. Делаем К шагов

![[Pasted image 20230407184318.png]]

На большом K x получится из настоящего распределения p_x

## Score mutching промежуточные итоги

![[Pasted image 20230407184426.png]]


## Нюансы score mutching
- мы ничего не знаем про регионы без данных
	- сеть может в этих регионах нарисовать все что угодно, находясь между мод распределения данных
	- можем уйти не туда
		![[Pasted image 20230407185357.png]]
- sliced score mutching - нестабильное обучение
	- Добавление шума стабилизирует обучение (изначально так как данные могут жить на многообразии небольшой размерности)

- Динамка Ланжевена плохо переходит между модами
	- В данных высокой размерности ей трудно переключаться между модами
		![[Pasted image 20230407185856.png]]
	-  Предположения динамики Ланжевена не работает

Зашумили мало - не обучимся, зашумим много - не то что хотели получим

### Несколько уровней зашумления
![[Pasted image 20230407190053.png]]

### Noise condition score network (NCSN) - обусловим score-функцию на уровень i
На каждом уровне хотим чтобы функция имела скор-функцию каждого определенного слоя
![[Pasted image 20230407190530.png]]

Обучаем:

(тупа сумма по всем слоям)

\
![[Pasted image 20230407190605.png]]

Домножаем на $\lambda(i)$, чтобы избавиться от зависимости от $\sigma_i^2$


- выбираем уровни зашумления
- Запускаем сначала самый высокий уровень зашумления (на слайде ошибка, начинаем с $i=L$)
- Новый уровень зашумления начинается там где закончился прошлый
- Запускаетвя динамика Ленжевена на каждом слое
- пока все очень зашемленно, берем больше шаги по Ленжевену
- Когда зашумления меньше, берем шаги поменьше

\


![[Pasted image 20230407190754.png]]


## [[DDPM]] - другая модель