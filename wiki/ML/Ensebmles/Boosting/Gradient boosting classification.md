---
created:
  - " 13-11-2023 18:32"
aliases: 
tags:
  - article/
---

# Gradient boostin classification

## Margin
![[Pasted image 20231113183240.png]]
- перемножаем истинный лейбл на его предсказание

- $M_i>0 \iff$ верная классификация
- $M_i<0 \iff$ - объект классифицируется неверно
- Чем больше модуль, тем больше уверенности

Функционал качества - число ошибок на обучении
![[Pasted image 20231113183349.png]]
**Недостаток** - неоптимизируемый


## Взвешивание объектов для задачи классификации

![[Pasted image 20231113183627.png]]
левую экспоненту можно интерпретировать как вес объекта

![[Pasted image 20231113183728.png]]

(по сути рпосто взяли выборку и домножили таргет на веса)
- при этом модель может излишне подстраиваться под шумы

## AdaBoost

![[Pasted image 20231113183828.png]]

Тогда минимум функционала достигается при:
![[Pasted image 20231113183905.png]]

## XGBoost
**Добавили инфу о вторых производных и регуляризатор**


### Пространство $\mathbf{R}^n$
При построении градиентног обустинга рассмотрим градиентный спуск в пространстве $\mathbf{R}^n$.

Соответствие обнозначений:

![[Pasted image 20231113184308.png]]

### Построение $b_t$
![[Pasted image 20231113184445.png]]
Распишем в терминаз пространства:


![[Pasted image 20231113184515.png]]

Почему задача логична? 
$s$ - предсказание предыдущей композиции, $\delta s$ - текущее то что добавляем
![[Pasted image 20231113184551.png]]

![[Pasted image 20231113184718.png]]
Задача оптимизации:
![[Pasted image 20231113184733.png]]

В обычном градиентном бустинге:
![[Pasted image 20231113184754.png]]

**Что сделали** - в бустинге считаем все вторые производные единицы (**аппроксимация второго порядка**), игнорируем кривизну функции потерь, в XGBoost - кажется что новая задача лучше приближает истинную функцию потерь


![[Pasted image 20231113184905.png]]

### Регуляризация

![[Pasted image 20231113185014.png]]

Введем регуляризаторы, штрафующие оба вида сложности:
![[Pasted image 20231113185138.png]]
Получили новую задачу оптимизации

### Как находить оптимальные ответы в листьях

Пусть дерево построено, найдем оптимальные ответы в листьях

![[Pasted image 20231113185458.png]]

Перепишем:
![[Pasted image 20231113185553.png]]

$G_k$ - сумма градиентов по объектам листа

Задача разбивается на $k$ независимых по листьям подзадач, оптимум:
![[Pasted image 20231113185743.png]]

Подставим и перепишем:
![[Pasted image 20231113185803.png]]

Новый $H(X_k)$ - критерий информативности

![[Pasted image 20231113190001.png]]
- тем самым выбираем структуру дерева исходя из минимизации исходной функции потерь

